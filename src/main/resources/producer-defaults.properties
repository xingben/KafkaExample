###
### Producer Basics
###

# The client id is a user-specified string sent in each request to help trace calls.  It should logically identify the
# application making the request.
#
client.id=test-producer

#A list of host/port pairs to use for establishing the initial connection to the Kafka cluster. 
#The client will make use of all servers irrespective of which servers are specified here for bootstrapping\u2014
#this list only impacts the initial hosts used to discover the full set of servers. 
#This list should be in the form host1:port1,host2:port2,.... 
#Since these servers are just used for the initial connection to discover the full cluster membership (which may change dynamically), 
#this list need not contain the full set of servers (you may want more than one, though, in case a server is down).
bootstrap.servers=benxing-linux3:9092,benxing-linux4:9092,benxing-linux2:9092

#Serializer class for key that implements the Serializer interface.
key.serializer=org.apache.kafka.common.serialization.StringSerializer

#Serializer class for value that implements the Serializer interface.
value.serializer=org.apache.kafka.common.serialization.StringSerializer

# The partitioner class for partitioning messages amongst sub-topics. The default partitioner is based on the hash of
# the key.
#
partitioner.class=org.apache.kafka.clients.producer.internals.DefaultPartitioner

#The number of acknowledgments the producer requires the leader to have received before considering a request complete. 
#This controls the durability of records that are sent. The following settings are common:
#acks=0 If set to zero then the producer will not wait for any acknowledgment from the server at all. 
		#The record will be immediately added to the socket buffer and considered sent. 
		#No guarantee can be made that the server has received the record in this case, 
		#and the retries configuration will not take effect (as the client won't generally know of any failures). 
		#The offset given back for each record will always be set to -1.
#acks=1 This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. 
		#In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.
#acks=all This means the leader will wait for the full set of in-sync replicas to acknowledge the record. 
		#This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. 
		#This is the strongest available guarantee.
acks=1


# This property will cause the producer to automatically retry a failed send request.  This property specifies the
# number of retries when such failures occur. Note that setting a non-zero value here can lead to duplicates in the case
# of network errors that cause a message to be sent but the acknowledgement to be lost.
retries=3

#The compression type for all data generated by the producer. 
#The default is none (i.e. no compression). Valid values are none, gzip, snappy, or lz4. 
#Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio 
#(more batching means better compression).
compression.type=snappy

#Close idle connections after the number of milliseconds specified by this config.
connections.max.idle.ms=60000

#The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. 
#This helps performance on both the client and the server. This configuration controls the default batch size in bytes.
#No attempt will be made to batch records larger than this size.

#Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent.

#A small batch size will make batching less common and may reduce throughput 
#(a batch size of zero will disable batching entirely). 
#A very large batch size may use memory a bit more wastefully 
#as we will always allocate a buffer of the specified batch size in anticipation of additional records.
batch.size=200

#The producer groups together any records that arrive in between request transmissions into a single batched request. 
#Normally this occurs only under load when records arrive faster than they can be sent out. 
#However in some circumstances the client may want to reduce the number of requests even under moderate load. 
#This setting accomplishes this by adding a small amount of artificial delay\u2014
#that is, rather than immediately sending out a record the producer will wait for up to the given delay 
#to allow other records to be sent so that the sends can be batched together. 
#This can be thought of as analogous to Nagle's algorithm in TCP. 
#This setting gives the upper bound on the delay for batching: 
#once we get batch.size worth of records for a partition it will be sent immediately 
#regardless of this setting, however if we have fewer than this many bytes accumulated for this 
#partition we will 'linger' for the specified time waiting for more records to show up. 
#This setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, 
#would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absense of load.
linger.ms=10

#When our memory buffer is exhausted we must either stop accepting new records (block) or throw errors.
#By default this setting is true and we block, however in some scenarios blocking is not desirable and it is better to immediately give an error. 
#Setting this to false will accomplish that: the producer will throw a BufferExhaustedException if a recrord is sent and the buffer space is full.
block.on.buffer.full=true
